---
layout: post
title: 
date: 2025-03-24
description: ðŸ¤” <b><i>  </b></i>
tags: meta
categories: news
---

### Meta
Llama models are being used to:
- [build an AI career coach to improve job search outcomes](https://jobsearchgenius.ai/)
- build a tourism app to learn about local history and culture
- [facilitate more efficient and accurate deals in the mergers and acquisitions (M&A) space](https://fynopsis.ai/)
From what I gathered, some prefer open-source models due to cost efficiency (fixed cost instead of paying per API call), data security and privacy (as the model can be fine-tuned locally), and a large developer community (making it easier to find solutions to problems). I am honestly very keen on trying them out as well.

### Google DeepMind
Google DeepMind introduced [two new robotics AI models](https://deepmind.google/technologies/gemini-robotics/), based on Gemini 2.0: Gemini Robotics (advanced vision-language-action (VLA) model) and Gemini Robotics-ER (advanced spatial understanding). They have released a [Tech Report](https://storage.googleapis.com/deepmind-media/gemini-robotics/gemini_robotics_report.pdf) for Gemini Robotics as well as a [new ASIMOV dataset](https://asimov-benchmark.github.io/) for evaluating and improving semantic safety in these robotic models. It's a fun read if you're into robotics!

### OpenAI
Researchers at the MIT Media Lab and OpenAI conducted a series of studies to understand how AI use that involves emotional engagement (they called it "affective use") can impact usersâ€™ well-being. They carried out two studies: [an observational study]((https://www.media.mit.edu/publications/investigating-affective-use-and-emotional-well-being-on-chatgpt/)) to analyze real-world on-platform usage patterns, and [a controlled interventional study](https://www.media.mit.edu/publications/how-ai-and-human-behaviors-shape-psychosocial-effects-of-chatbot-use-a-longitudinal-controlled-study/) to understand the impacts on users. I've always been fascinated by psychology and behavioral sciences, so this study truly captivates me!

### Microsoft
Microsoft released a playbook, [Accelerating sustainability with AI: Innovations for a better future](https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/presentations/CSR/Accelerating-Sustainability-with-AI-2025.pdf), outlining 5 ways to advance sustainability. I think it's an interesting read for anyone interested in integrating AI into their business.

### NVIDIA
NVIDIA released [Open Physical AI Dataset](https://huggingface.co/collections/nvidia/physical-ai-67c643edbb024053dcbcd6d8) (available on Hugging Face) for robotics and autonomous vehicle development. This dataset will continue to grow over time and will include both real-world and synthetic data. NVIDIA also will be using this dataset to train, test and validate physical AI for the [NVIDIA Cosmos](https://nvidianews.nvidia.com/news/nvidia-announces-major-release-of-cosmos-world-foundation-models-and-physical-ai-data-tools) world model development platform, the [NVIDIA DRIVE AV](https://www.nvidia.com/en-us/self-driving-cars/in-vehicle-computing/) software stack, the [NVIDIA Isaac](https://www.nvidia.com/en-us/industries/robotics/) AI robot development platform and the [NVIDIA Metropolis](https://www.nvidia.com/en-us/autonomous-machines/intelligent-video-analytics-platform/) application framework for smart cities. Iâ€™d love to dive deeper into each one of them, they sound extremely fascinating!

### IBM
IBM released [Bee AI](https://beeai.dev/) which is an open platform to run popular open-source AI agents from different frameworks. It can also be used to build specialized agents and be configured to work alone or with AI teammates. They also introduced [agent communication protocol](https://docs.beeai.dev/acp/alpha/introduction) (ACP) to standardize how agents talk to each other. This is a step ahead of Anthropicâ€™s [model context protocol](https://modelcontextprotocol.io/introduction) (MCP) that standardizes how agents connect to tools and data to interact with and accomplish tasks in the real world. I think such protocol is important when developing multiagent systems, which are likely the next big advancement.

### Anthropic
Anthropic released a paper: [Auditing Language Models for Hidden Objectives](https://assets.anthropic.com/m/317564659027fb33/original/Auditing-Language-Models-for-Hidden-Objectives.pdf). In the paper, they studied the feasibilit of conducting alignment audits which are systematic investigations into whether models are pursuing hidden objectives. The objective is to uncover whether some AI systems that appear well-behaved actually harbor secret motives that are potentially misaligned with our intent. This is definitely something that I'll read up more in detail as I'm very much into AI safety!

### References
- [Our open source Llama models are helping to spur economic growth in the US](https://ai.meta.com/blog/built-with-llama-writesea-fynopsis-srimoyee-mukhopadhyay-united-states-economy/)
- [Gemini Robotics brings AI into the physical world](https://deepmind.google/discover/blog/gemini-robotics-brings-ai-into-the-physical-world/)
- [Early methods for studying affective use and emotional well-being on ChatGPT](https://openai.com/index/affective-use-study/)
- [Harnessing AI for resilience, efficiency, and sustainability](https://www.microsoft.com/en-us/microsoft-cloud/blog/2025/03/18/harnessing-ai-for-resilience-efficiency-and-sustainability/)
- [NVIDIA Unveils Open Physical AI Dataset to Advance Robotics and Autonomous Vehicle Development](https://blogs.nvidia.com/blog/open-physical-ai-dataset/)
- [BeeAI now has multiple agents, and a standardized way for them to talk](https://research.ibm.com/blog/multiagent-bee-ai)
- [Auditing language models for hidden objectives](https://www.anthropic.com/research/auditing-hidden-objectives)