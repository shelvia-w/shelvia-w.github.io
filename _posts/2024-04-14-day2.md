---
layout: post
title: Day 2 of Learning
date: 2024-04-13
description: Measure Theory, Grokking, player2vec, Information in DNNs
tags: measure-theory, grokking, player2vec, information-theory
categories: challenge
---

This is day 2 of the challenge to read/code more. Here's a summary of what I learnt today:

**50 Challenging Problems in Probability: The Hurried Duelers**

I published an article on [Medium](https://medium.com/@shelvia1039/50-challenging-problems-in-probability-part-26-the-hurried-duelers-28b0b4aa5d7e) about the hurried duelers. The problem is about finding the probability that two duelers meet given a certain condition.

**Measure Theory**

I was thinking of what math topic to start learning, and decided on "Measure Theory". The Bright Side of Mathematics channel on Youtube has a [playlist](https://www.youtube.com/watch?v=FtEmLexUw3Y&list=PLBh2i93oe2quIJS-j1NpbzEvQCmN00F5o) on Measure Theory. I've only managed to watched three videos so far, but I learned a few new terms such as measurable set, sigma algebras, Borel sigma algebras, and measure itself. I hope after completing this playlist, I'll be able to understand some notations and terms better in research papers.

**Grokking**

My advisor recently shared with me an [article](https://www.quantamagazine.org/how-do-machines-grok-data-20240412/) on Quanta Magazine on 'grokking'. The term grokking is used to describe a phenomenon in deep learning where the models exhibit good generalization performance long after the overfitting phase kicks in. My friend who went to NeurIPS 2022 has shared the paper ["Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets"](https://arxiv.org/pdf/2201.02177.pdf) with me last year but I didn't have the chance to read it. It seems that there are a few interesting follow-up works, which try to explain this phenomenon and even attempt to connect it with the double descent phenomenon. If I have the time, I would also like to research about this phenomenon more from the information theory perspective.

**player2vec**

I read a new [article](https://www.marktechpost.com/2024/04/13/unveiling-player-insights-a-novel-machine-learning-approach-to-understanding-gaming-behavior/) on MarkTechPost that talked about this recently published paper called player2vec(https://arxiv.org/pdf/2404.04234.pdf). It immediately caught my attention as it's about understanding player's gaming behaviour. The paper itself did not explicitly mention which mobile game they are analyzing, which makes me even more curious. I did not read the paper in detail but I found the results of their cluster analysis rather interesting. I'll read the paper fully when I have the time and maybe try to figure out which mobile game they are analyzing haha.

**Information in DNNs**

I've been wanting to understand more deeply about how deep learning is formulated mathematically from the perspective of information theory. One paper that talks about this is [Where is the Information in a Deep Neural Network?](https://arxiv.org/pdf/1905.12213.pdf) which was written by Alessandro Achille, Giovanni Paolini, and Stefano Soatto. I've heard of Achille and Soatto, and I think they have a few papers that apply information theory to deep learning. Shockingly, the paper was rejected at ICLR 2020 despite its positive reviews. The paper defines the information in the weights using an arbitrary "pre-distribution" and "post-distribution". They also relate the different choices of pre- and post-distributions to Shannon's mutual information and Fisher Information. Finally, they relate the information in the weights to the information in the activations, which allow them to talk about generalization and invariance. In particular, they concluded that a network with minimal information in the weights is forced to learn a representation that is effectively invariant to nuisances. I haven't fully grasped the theory in the paper yet, so I'm gonna re-read it next time.