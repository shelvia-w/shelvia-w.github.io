<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://shelvia-w.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shelvia-w.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-04-16T16:38:50+00:00</updated><id>https://shelvia-w.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Day 1 of Learning</title><link href="https://shelvia-w.github.io/blog/2024/day1/" rel="alternate" type="text/html" title="Day 1 of Learning"/><published>2024-04-13T00:00:00+00:00</published><updated>2024-04-13T00:00:00+00:00</updated><id>https://shelvia-w.github.io/blog/2024/day1</id><content type="html" xml:base="https://shelvia-w.github.io/blog/2024/day1/"><![CDATA[<p>This is day 1 of the challenge to read/code more. I realized that this challenge is harder than I thought. But it’s kinda fun as I get to learn about so many interesting topics. Here’s a summary of what I learnt today:</p> <p><strong>Brain teaser: Have we met before?</strong></p> <p>I published an article on <a href="https://medium.com/@shelvia1039/brain-teaser-26-have-we-met-before-cafd776b81d4">Medium</a> today about the theorem of friends and strangers. There’s actually a <a href="https://en.wikipedia.org/wiki/Theorem_on_friends_and_strangers#:~:text=The%20theorem%20on%20friends%20and,blue%20colours%20of%20each%20graph.">Wikipedia</a> page about it. The solution to this problem is related to <a href="https://en.wikipedia.org/wiki/Ramsey%27s_theorem">Ramsey’s theorem</a>.</p> <p><strong>LeetCode: Longest substring without repeating characters</strong></p> <p>I did a <a href="https://leetcode.com/problems/longest-substring-without-repeating-characters/description/">LeetCode</a> problem about finding the longest substring without repeating characters. I managed to solve it but was not quite satisfied with my solution. The given solution uses sliding window, which is an important concept in algorithms (I think). I am not too familiar with data structures and algorithms, so doing LeetCode gives me the opportunity to learn about them. Solving LeetCode is kinda like solving math puzzles as well, but currently I’m just doing easy problems to understand the common algorithms.</p> <p><strong>OpenAI: Improving their fine-tuning API and custom models program</strong></p> <p>OpenAI published in their <a href="https://openai.com/blog/introducing-improvements-to-the-fine-tuning-api-and-expanding-our-custom-models-program">blog</a> that they have new features for their fine-tuning API, including epoch-based checkpoint creation, comparative playground, third-party integration, comprehensive validation metrics, hyperparameter configuration and fine-tuning dashboard improvements. They are also expanding their custom models program to include assisted fine-tuning where you can work with their technical teams to fine-tuning the models more deeply to suit your interests. If you have large proprietary data, you may also work with them to integrate the domain knowledge better into the model for very specific use cases. I would love to read more about their retrieval-augmented generation (RAG), fine-tuning API and custom models program in the future.</p> <p><strong>Generalization in deep learning</strong></p> <p>I’ve been working with the topic of generalization in deep learning for a while now. But I’ve been struggling to consolidate all of them, and I haven’t been keeping up with the latest development. So I thought I should start somewhere and decided to start with a paper which I’ve been wanting to read: <a href="https://arxiv.org/pdf/1710.05468.pdf">Generalization in Deep Learning</a>. I’ve only read the Introduction so far, and here’s what I gather:</p> <ul> <li>Theoretical properties of neural networks include: expressivity and non-convex optimization (how trainable the deep hypothesis spaces are)</li> <li>Expressive and trainable hypothesis space does not guarantee generalization (due to possibility of overfitting)</li> <li>Classical theory suggests that: (a) generalization has someething to do with low-capacity hypothesis class (compact representation), and (b) deep hypothesis spaces have exponential advantage over shallow ones (need to read and understand this further)</li> <li>However, it has been shown that successful deep hypothesis spaces have sufficient capacity to memorize random labels (apparent paradox)</li> <li>Question: why do deep learning models generalize well despite their overwhelming capacity?</li> </ul> <p><strong>TMLR (Nov 2023 - Mar 2024) and JMLR (Vol 25)</strong></p> <p>I picked some papers from <a href="https://jmlr.org/tmlr/papers/">TMLR</a> (Nov 2023 - Mar 2024) and <a href="https://jmlr.org/papers/v25/">JMLR</a> (Vol 25) which I find interesting. I found that there are quite a few papers analyzing the theoretical properties of gradient descent-based methods. Here are the top 3 papers which caught my attention:</p> <ul> <li><a href="https://jmlr.org/papers/volume25/19-301/19-301.pdf">On Truthing Issues in Supervised Classification</a></li> <li><a href="https://openreview.net/pdf?id=EWv9XGOpB3">Variational Classification: A Probabilistic Generalization of the Softmax Classifier</a></li> <li><a href="https://openreview.net/pdf?id=TQfQUksaC8">Pathologies of Predictive Diversity in Deep Ensembles</a></li> </ul>]]></content><author><name></name></author><category term="challenge"/><category term="leetcode,"/><category term="openai,"/><category term="generalization,"/><category term="jmlr,"/><category term="tmlr"/><summary type="html"><![CDATA[LeetCode, OpenAI, Generalization, TMLR/JMLR]]></summary></entry><entry><title type="html">Day 2 of Learning</title><link href="https://shelvia-w.github.io/blog/2024/day2/" rel="alternate" type="text/html" title="Day 2 of Learning"/><published>2024-04-13T00:00:00+00:00</published><updated>2024-04-13T00:00:00+00:00</updated><id>https://shelvia-w.github.io/blog/2024/day2</id><content type="html" xml:base="https://shelvia-w.github.io/blog/2024/day2/"><![CDATA[<p>This is day 2 of the challenge to read/code more. Here’s a summary of what I learnt today:</p> <p><strong>50 Challenging Problems in Probability: The Hurried Duelers</strong></p> <p>I published an article on <a href="https://medium.com/@shelvia1039/50-challenging-problems-in-probability-part-26-the-hurried-duelers-28b0b4aa5d7e">Medium</a> about the hurried duelers. The problem is about finding the probability that two duelers meet given a certain condition.</p> <p><strong>Measure Theory</strong></p> <p>I was thinking of what math topic to start learning, and decided on “Measure Theory”. The Bright Side of Mathematics channel on Youtube has a <a href="https://www.youtube.com/watch?v=FtEmLexUw3Y&amp;list=PLBh2i93oe2quIJS-j1NpbzEvQCmN00F5o">playlist</a> on Measure Theory. I’ve only managed to watched three videos so far, but I learned a few new terms such as measurable set, sigma algebras, Borel sigma algebras, and measure itself. I hope after completing this playlist, I’ll be able to understand some notations and terms better in research papers.</p> <p><strong>Grokking</strong></p> <p>My advisor recently shared with me an <a href="https://www.quantamagazine.org/how-do-machines-grok-data-20240412/">article</a> on Quanta Magazine on ‘grokking’. The term grokking is used to describe a phenomenon in deep learning where the models exhibit good generalization performance long after the overfitting phase kicks in. My friend who went to NeurIPS 2022 has shared the paper <a href="https://arxiv.org/pdf/2201.02177.pdf">“Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets”</a> with me last year but I didn’t have the chance to read it. It seems that there are a few interesting follow-up works, which try to explain this phenomenon and even attempt to connect it with the double descent phenomenon. If I have the time, I would also like to research about this phenomenon more from the information theory perspective.</p> <p><strong>player2vec</strong></p> <p>I read a new <a href="https://www.marktechpost.com/2024/04/13/unveiling-player-insights-a-novel-machine-learning-approach-to-understanding-gaming-behavior/">article</a> on MarkTechPost that talked about this recently published paper called player2vec(https://arxiv.org/pdf/2404.04234.pdf). It immediately caught my attention as it’s about understanding player’s gaming behaviour. The paper itself did not explicitly mention which mobile game they are analyzing, which makes me even more curious. I did not read the paper in detail but I found the results of their cluster analysis rather interesting. I’ll read the paper fully when I have the time and maybe try to figure out which mobile game they are analyzing haha.</p> <p><strong>Information in DNNs</strong></p> <p>I’ve been wanting to understand more deeply about how deep learning is formulated mathematically from the perspective of information theory. One paper that talks about this is <a href="https://arxiv.org/pdf/1905.12213.pdf">Where is the Information in a Deep Neural Network?</a> which was written by Alessandro Achille, Giovanni Paolini, and Stefano Soatto. I’ve heard of Achille and Soatto, and I think they have a few papers that apply information theory to deep learning. Shockingly, the paper was rejected at ICLR 2020 despite its positive reviews. The paper defines the information in the weights using an arbitrary “pre-distribution” and “post-distribution”. They also relate the different choices of pre- and post-distributions to Shannon’s mutual information and Fisher Information. Finally, they relate the information in the weights to the information in the activations, which allow them to talk about generalization and invariance. In particular, they concluded that a network with minimal information in the weights is forced to learn a representation that is effectively invariant to nuisances. I haven’t fully grasped the theory in the paper yet, so I’m gonna re-read it next time.</p>]]></content><author><name></name></author><category term="challenge"/><category term="measure"/><category term="theory,"/><category term="grokking,"/><category term="player2vec,"/><category term="information"/><category term="theory"/><summary type="html"><![CDATA[Measure Theory, Grokking, player2vec, Information in DNNs]]></summary></entry><entry><title type="html">New Challenge</title><link href="https://shelvia-w.github.io/blog/2024/new-challenge/" rel="alternate" type="text/html" title="New Challenge"/><published>2024-04-12T00:00:00+00:00</published><updated>2024-04-12T00:00:00+00:00</updated><id>https://shelvia-w.github.io/blog/2024/new-challenge</id><content type="html" xml:base="https://shelvia-w.github.io/blog/2024/new-challenge/"><![CDATA[<p>This is rather random, but as my PhD journey is coming close to an end, I would like to challenge myself with reading and coding. I am partially unsatisfied with where I am right now, but I think it’s not necessarily a bad thing, as it motivates me to improve myself. So, I will take a little bit of time every day to read and/or code things that I am interested in. In the field of AI, I am most interested in how to build trustworthy AI, so this includes topics like fairness, explainability, privacy, etc. But I’m also interested in other fields, such as particle physics and neuroscience. So I should challenge myself to improve my knowledge in those fields as well. I will try my best to log my journey daily, but I don’t want to put too much pressure on myself. I’ll keep reminding myself that something is better than nothing, and at least I’m slowly leveling up.</p>]]></content><author><name></name></author><category term="challenge"/><category term="challenge,"/><category term="reading,"/><category term="coding"/><summary type="html"><![CDATA[A new challenge I set up for myself.]]></summary></entry></feed>