<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://shelvia-w.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shelvia-w.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-05-07T08:23:48+00:00</updated><id>https://shelvia-w.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Advice from Sholto Douglas and Trenton Bricken</title><link href="https://shelvia-w.github.io/blog/2024/advice1/" rel="alternate" type="text/html" title="Advice from Sholto Douglas and Trenton Bricken"/><published>2024-04-19T00:00:00+00:00</published><updated>2024-04-19T00:00:00+00:00</updated><id>https://shelvia-w.github.io/blog/2024/advice1</id><content type="html" xml:base="https://shelvia-w.github.io/blog/2024/advice1/"><![CDATA[<iframe width="560" height="315" src="https://www.youtube.com/embed/cPu3SecmgUU?si=ViEzK-OiBV-v9CC6" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> <p>I watched an interview with Sholto Douglas (AI researcher at Google DeepMind) and Trenton Bricken (AI researcher at Anthropic), and have been greatly inspired by what they said. Here are some reflections:</p> <h3 id="many-ideas-need-execution">Many Ideas, Need Execution</h3> <p>Trenton mentioned that he and his team constantly have many ideas, but all these ideas need careful, thorough, and quick investigation. I totally relate to this point. Sometimes when my labmates and I discuss some problems, we come up with a bunch of ideas on what kind of interesting analysis we can do. Sadly, many of these ideas stay as ideas since neither of us has the time nor energy to execute them. There‚Äôs no excuse for this, of course. I have to get better at quickly executing any good ideas, which is a very important trait to have (Sholto called it the agency at work). Good ideas are nothing without proper execution. The implementation also needs to be fast (without sacrificing correctness) because many good ideas are lining up, waiting to be tested out there.</p> <h3 id="pursue-a-problem-till-the-end">Pursue a Problem till the End</h3> <p>Both Trenton and Sholto agreed that it‚Äôs very important for a good researcher to have the dedication to pursue a problem until the end. If you encounter a problem, don‚Äôt simply give up or get stuck on it for a long period of time. Instead, really delve deep into the problem itself and conduct your own research on how to potentially solve it. Maybe talk to people or different experts to get some ideas on how to overcome each problem. No matter what, there‚Äôs no excuse for just saying you are not good enough or the problem is simply too hard. Even if it‚Äôs a technical issue that you don‚Äôt have expertise in, you can still learn about it and potentially acquire new (and useful) skills after solving it. So don‚Äôt give up!</p> <h3 id="getting-good-at-picking-problems">Getting Good at Picking Problems</h3> <p>Sholto mentioned that one of the ways he has been impactful in his team is by being very good at picking problems that haven‚Äôt been well-solved. Although the task of picking a problem to solve might seem easy, it actually requires a lot of experience. If you pick a problem that is very ‚Äúhot‚Äù in the field right now, you are competing with the very best, and they potentially have more resources to solve the problem. On the other hand, if you pick a relatively unexplored but important problem to solve and manage to solve it, that alone can make you famous. This again requires tenacity and dedication to stick to solving the problem, no matter how hard it gets. The ability to pick a hard but solvable problem requires one to read widely and build intuition about what works and what doesn‚Äôt.</p> <h3 id="manufacture-your-own-luck">Manufacture Your Own Luck</h3> <p>Being recognized as world-class AI researchers is all about being in the right place at the right time. You could call it luck. But truthfully, you can manufacture your own luck (or manifest it better) by putting yourself out there. Engaging in independent learning, attending conferences, and networking with various researchers greatly improve your chances of of being recognized. When you do some independent learning outside your scope of work, you are training yourself to become a general problem solver. You could potentially offer a fresh perspective on looking at a problem based on your experience from a different field. This also allows you to talk to a wide variety of researchers and opens up more opportunities for you.</p> <h3 id="caring-unbelievable-amount">Caring Unbelievable Amount</h3> <p>Sholto mentioned at the end of the video that above everything else, the most important attitude to have is caring an unbelievable amount about a problem. When you do, you don‚Äôt give up on the problems easily, you try to think of any possible solutions to overcome any obstacles that you encounter, and you make sure the investigations you carry out are thorough and correct. You naturally want to improve yourself so that you can be better at solving these problems, and you also become more detail-oriented, carefully checking that every step of the implementation is correct. I guess this is equivalent to being passionate about what you are doing, and many good AI researchers undoubtedly have this quality.</p>]]></content><author><name></name></author><category term="advice"/><category term="advice,"/><category term="work,"/><category term="motivation"/><summary type="html"><![CDATA[ü§î How to be world-class AI researcher?]]></summary></entry><entry><title type="html">De-optimize Your Life with Downtime</title><link href="https://shelvia-w.github.io/blog/2024/downtime/" rel="alternate" type="text/html" title="De-optimize Your Life with Downtime"/><published>2024-04-19T00:00:00+00:00</published><updated>2024-04-19T00:00:00+00:00</updated><id>https://shelvia-w.github.io/blog/2024/downtime</id><content type="html" xml:base="https://shelvia-w.github.io/blog/2024/downtime/"><![CDATA[<iframe width="560" height="315" src="https://www.youtube.com/embed/cqdm9z3oF0c?si=IRpkhJb8T2ayQfbw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> <p>In the above video, Maria Cano talked about the importance of having downtime. Here‚Äôs a summary:</p> <h3 id="our-brains-need-rest">Our Brains Need Rest</h3> <p>When our brain switches from task to task, it burns up oxygenated glucose at a faster rate than usual. However, we need this glucose to learn new things, remember things, and stay focused. Ironically, when we fill our day with back-to-back tasks, hoping to get more things done, we might end up becoming less productive as our brain starts to run out of fuel.</p> <h3 id="what-is-downtime">What is Downtime?</h3> <p>Downtime is unstructured time with no targeted goal and no focused attention. Scrolling on social media, watching videos, listening to music, and even meditating are not considered downtime as you would have a specific goal in mind and require focus of attention. Downtime is really about letting your mind wander without any specific purpose, almost like daydreaming.</p> <h3 id="benefits-of-downtime">Benefits of Downtime</h3> <p>Having downtime can lead to more creativity, allowing your brain to make unexpected connections and giving rise to new ideas. It can also give rise to sudden realizations, or what some people call ‚Äòaha‚Äô moments. You might suddenly remember something important or find the solutions you need for your problems. With the rise of artificial intelligence to help boost our productivity, creativity in the workplace will be valued even more than before.</p> <p><strong>Takeaway: remember to take short breaks (no phone, go for a walk, drink tea and stare at nothing) in between tasks!</strong></p> <h3 id="some-reflections">Some Reflections</h3> <p>I wholeheartedly agree with the importance of having downtime. Sometimes, my phone would run out of battery while I was outside, and I was forced to do nothing and let my brain rest. Yes, it‚Äôs often an unintended situation; I almost never intentionally include downtime in my daily schedule. But it is when my brain starts to wander aimlessly that I start getting interesting ideas and even find a way to solve some of my problems. Downtime really helps my brain to organize the information in my head, which would otherwise be a jumbled mess. In the next article, I talked about how important it is to organize the information in your head (summary: it enables you to express your thoughts more clearly to others). I wonder if it feels weird or awkward to intentionally set aside some downtime in my daily life. Maybe I should try to have some downtime after every meal as that‚Äôs the period of time where I‚Äôm most unproductive.</p>]]></content><author><name></name></author><category term="advice"/><category term="advice,"/><category term="work,"/><category term="motivation"/><summary type="html"><![CDATA[üò¥ Having downtime is critical for creativity.]]></summary></entry><entry><title type="html">How to Think Clearly?</title><link href="https://shelvia-w.github.io/blog/2024/thinking-clearly/" rel="alternate" type="text/html" title="How to Think Clearly?"/><published>2024-04-19T00:00:00+00:00</published><updated>2024-04-19T00:00:00+00:00</updated><id>https://shelvia-w.github.io/blog/2024/thinking-clearly</id><content type="html" xml:base="https://shelvia-w.github.io/blog/2024/thinking-clearly/"><![CDATA[<iframe width="560" height="315" src="https://www.youtube.com/embed/WzsPAmeDykw?si=xIs1sWa23IrsJbgh" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> <p>In the above video, Joseph Tsar talked about how to think clearly. Here‚Äôs a summary:</p> <h3 id="consume-less-bite-sized-media">Consume Less Bite-Sized Media</h3> <p>Short-form content (TikTok, YouTube Shorts, Instagram Reels) is undoubtedly the current trend now. It can cater to anyone who has just few minutes of free time to spare due to their busy schedule. However, this leads to us having a shorter attention span. We start to find long videos boring, even though they could potentially be more useful. So it‚Äôs important to retrain our brains to maintain a longer attention span and absorb information from longer videos or articles.</p> <h3 id="exercise-mental-discipline">Exercise Mental Discipline</h3> <p>There‚Äôs so much information to consume on the Internet, and this could potentially lead to us being ‚Äúlazy‚Äù. We start parroting what others are saying without really processing and evaluating the information itself. As Joseph mentioned, this leads to a ‚Äúsociety full of people who have a very surface-level understanding of often very strong opinions‚Äù. We have to do the due diligence of researching and internalizing the pre-processed information instead of blindly adopting someone else‚Äôs understanding of it (which may not necessarily be correct). This would also train our thought process, allowing us to express idea more clearly to others (since we have gone through the step-by-step process of arriving at the conclusion in our heads). We would have more confidence in delivering the ideas as we have a better understanding of them. When we just blindly follow someone else‚Äôs understanding, we find it harder to defend it ourselves as our understanding becomes limited to what we heard from that person who shares it. So whenever we find our brains being lazy, remember to confront ourselves and start thinking through all the ideas in our heads. If you find yourself disagreeing or challenging someone else‚Äôs understanding, it also shows that you are actively using your brain to evaluate the information. We could potentially help correct their misunderstanding or they could potentially help correct our misunderstanding (or sometimes both can be correct and you get a fresh perspective). Either way, it‚Äôs a win-win for society!</p> <h3 id="more-information-output">More Information Output</h3> <p>We are constantly consuming information on a daily basis, but we don‚Äôt output it enough. This leads to information overload in our brains, which manifests as brain fog. Some people unconsciously sort and process this information in their heads by having an internal dialogue with themselves. But for most of us, we need a way to output information to organize all the scattered ideas in our brains and process them clearly. Having a conversation with others is arguably the best way to output information because you could receive feedback that might alter your understanding or opinion about something. The more time we talk about a certain topic with others, the more confident we become, as each conversation reinforces our understanding of the matter. Writing about the topic or making videos about it is another popular way to output information. It forces us to strengthen our chain of thoughts and discover the best way to deliver the message to others.</p> <h3 id="some-reflections">Some Reflections</h3> <p>I am amazed at how accurately Joseph describes the problem. I guess he has done his due diligence in processing the information, which enables him to deliver the idea in the best way possible. It‚Äôs something I have thought about in the past, but I still struggle to articulate my opinions sometimes. I realize that I like to express my thoughts a lot and sometimes even feel a little embarrassed for talking about them too much when the receiving end is not as interested in the topic. That‚Äôs why I find joy in writing and journaling, where I can sort through the information and ideas that I have. I don‚Äôt consider myself a good writer though, as I find my writing rather boring (also English is not my native language). But I would not let that stop me from expressing my thoughts through the written form, and I believe it will improve the more I write.</p>]]></content><author><name></name></author><category term="advice"/><category term="advice,"/><category term="work,"/><category term="motivation"/><summary type="html"><![CDATA[üì¢ Why we are unable to express our ideas or thoughts clearly...]]></summary></entry><entry><title type="html">Day 2 of Learning</title><link href="https://shelvia-w.github.io/blog/2024/day2/" rel="alternate" type="text/html" title="Day 2 of Learning"/><published>2024-04-14T00:00:00+00:00</published><updated>2024-04-14T00:00:00+00:00</updated><id>https://shelvia-w.github.io/blog/2024/day2</id><content type="html" xml:base="https://shelvia-w.github.io/blog/2024/day2/"><![CDATA[<p>This is day 2 of the challenge to read/code more. Here‚Äôs a summary of what I learnt today:</p> <p><strong>Measure Theory</strong></p> <p>I was thinking of what math topic to start learning, and decided on ‚ÄúMeasure Theory‚Äù. The Bright Side of Mathematics channel on Youtube has a <a href="https://www.youtube.com/watch?v=FtEmLexUw3Y&amp;list=PLBh2i93oe2quIJS-j1NpbzEvQCmN00F5o">playlist</a> on Measure Theory. I‚Äôve only managed to watched three videos so far, but I learned a few new terms such as measurable set, sigma algebras, Borel sigma algebras, and measure itself. I hope after completing this playlist, I‚Äôll be able to understand some notations and terms better in research papers.</p> <p><strong>Grokking</strong></p> <p>My advisor recently shared with me an <a href="https://www.quantamagazine.org/how-do-machines-grok-data-20240412/">article</a> on Quanta Magazine on ‚Äògrokking‚Äô. The term grokking is used to describe a phenomenon in deep learning where the models exhibit good generalization performance long after the overfitting phase kicks in. My friend who went to NeurIPS 2022 has shared the paper <a href="https://arxiv.org/pdf/2201.02177.pdf">‚ÄúGrokking: Generalization Beyond Overfitting on Small Algorithmic Datasets‚Äù</a> with me last year but I didn‚Äôt have the chance to read it. It seems that there are a few interesting follow-up works, which try to explain this phenomenon and even attempt to connect it with the double descent phenomenon. If I have the time, I would also like to research about this phenomenon more from the information theory perspective.</p> <p><strong>player2vec</strong></p> <p>I read a new <a href="https://www.marktechpost.com/2024/04/13/unveiling-player-insights-a-novel-machine-learning-approach-to-understanding-gaming-behavior/">article</a> on MarkTechPost that talked about this recently published paper called player2vec(https://arxiv.org/pdf/2404.04234.pdf). It immediately caught my attention as it‚Äôs about understanding player‚Äôs gaming behaviour. The paper itself did not explicitly mention which mobile game they are analyzing, which makes me even more curious. I did not read the paper in detail but I found the results of their cluster analysis rather interesting. I‚Äôll read the paper fully when I have the time and maybe try to figure out which mobile game they are analyzing haha.</p> <p><strong>Information in DNNs</strong></p> <p>I‚Äôve been wanting to understand more deeply about how deep learning is formulated mathematically from the perspective of information theory. One paper that talks about this is <a href="https://arxiv.org/pdf/1905.12213.pdf">Where is the Information in a Deep Neural Network?</a> which was written by Alessandro Achille, Giovanni Paolini, and Stefano Soatto. I‚Äôve heard of Achille and Soatto, and I think they have a few papers that apply information theory to deep learning. Shockingly, the paper was rejected at ICLR 2020 despite its positive reviews. The paper defines the information in the weights using an arbitrary ‚Äúpre-distribution‚Äù and ‚Äúpost-distribution‚Äù. They also relate the different choices of pre- and post-distributions to Shannon‚Äôs mutual information and Fisher Information. Finally, they relate the information in the weights to the information in the activations, which allow them to talk about generalization and invariance. In particular, they concluded that a network with minimal information in the weights is forced to learn a representation that is effectively invariant to nuisances. I haven‚Äôt fully grasped the theory in the paper yet, so I‚Äôm gonna re-read it next time.</p>]]></content><author><name></name></author><category term="challenge"/><category term="measure-theory,"/><category term="grokking,"/><category term="player2vec,"/><category term="information-theory"/><summary type="html"><![CDATA[üí≠ Measure Theory, Grokking, player2vec, Information in DNNs]]></summary></entry><entry><title type="html">Day 1 of Learning</title><link href="https://shelvia-w.github.io/blog/2024/day1/" rel="alternate" type="text/html" title="Day 1 of Learning"/><published>2024-04-13T00:00:00+00:00</published><updated>2024-04-13T00:00:00+00:00</updated><id>https://shelvia-w.github.io/blog/2024/day1</id><content type="html" xml:base="https://shelvia-w.github.io/blog/2024/day1/"><![CDATA[<p>This is day 1 of the challenge to read/code more. I realized that this challenge is harder than I thought. But it‚Äôs kinda fun as I get to learn about so many interesting topics. Here‚Äôs a summary of what I learnt today:</p> <p><strong>LeetCode: Longest substring without repeating characters</strong></p> <p>I did a <a href="https://leetcode.com/problems/longest-substring-without-repeating-characters/description/">LeetCode</a> problem about finding the longest substring without repeating characters. I managed to solve it but was not quite satisfied with my solution. The given solution uses sliding window, which is an important concept in algorithms (I think). I am not too familiar with data structures and algorithms, so doing LeetCode gives me the opportunity to learn about them. Solving LeetCode is kinda like solving math puzzles as well, but currently I‚Äôm just doing easy problems to understand the common algorithms.</p> <p><strong>OpenAI: Improving their fine-tuning API and custom models program</strong></p> <p>OpenAI published in their <a href="https://openai.com/blog/introducing-improvements-to-the-fine-tuning-api-and-expanding-our-custom-models-program">blog</a> that they have new features for their fine-tuning API, including epoch-based checkpoint creation, comparative playground, third-party integration, comprehensive validation metrics, hyperparameter configuration and fine-tuning dashboard improvements. They are also expanding their custom models program to include assisted fine-tuning where you can work with their technical teams to fine-tuning the models more deeply to suit your interests. If you have large proprietary data, you may also work with them to integrate the domain knowledge better into the model for very specific use cases. I would love to read more about their retrieval-augmented generation (RAG), fine-tuning API and custom models program in the future.</p> <p><strong>Generalization in deep learning</strong></p> <p>I‚Äôve been working with the topic of generalization in deep learning for a while now. But I‚Äôve been struggling to consolidate all of them, and I haven‚Äôt been keeping up with the latest development. So I thought I should start somewhere and decided to start with a paper which I‚Äôve been wanting to read: <a href="https://arxiv.org/pdf/1710.05468.pdf">Generalization in Deep Learning</a>. I‚Äôve only read the Introduction so far, and here‚Äôs what I gather:</p> <ul> <li>Theoretical properties of neural networks include: expressivity and non-convex optimization (how trainable the deep hypothesis spaces are)</li> <li>Expressive and trainable hypothesis space does not guarantee generalization (due to possibility of overfitting)</li> <li>Classical theory suggests that: (a) generalization has someething to do with low-capacity hypothesis class (compact representation), and (b) deep hypothesis spaces have exponential advantage over shallow ones (need to read and understand this further)</li> <li>However, it has been shown that successful deep hypothesis spaces have sufficient capacity to memorize random labels (apparent paradox)</li> <li>Question: why do deep learning models generalize well despite their overwhelming capacity?</li> </ul> <p><strong>TMLR (Nov 2023 - Mar 2024) and JMLR (Vol 25)</strong></p> <p>I picked some papers from <a href="https://jmlr.org/tmlr/papers/">TMLR</a> (Nov 2023 - Mar 2024) and <a href="https://jmlr.org/papers/v25/">JMLR</a> (Vol 25) which I find interesting. I found that there are quite a few papers analyzing the theoretical properties of gradient descent-based methods. Here are the top 3 papers which caught my attention:</p> <ul> <li><a href="https://jmlr.org/papers/volume25/19-301/19-301.pdf">On Truthing Issues in Supervised Classification</a></li> <li><a href="https://openreview.net/pdf?id=EWv9XGOpB3">Variational Classification: A Probabilistic Generalization of the Softmax Classifier</a></li> <li><a href="https://openreview.net/pdf?id=TQfQUksaC8">Pathologies of Predictive Diversity in Deep Ensembles</a></li> </ul>]]></content><author><name></name></author><category term="challenge"/><category term="leetcode,"/><category term="openai,"/><category term="generalization,"/><category term="jmlr,"/><category term="tmlr"/><summary type="html"><![CDATA[üí≠ LeetCode, OpenAI, Generalization, TMLR/JMLR]]></summary></entry><entry><title type="html">New Challenge</title><link href="https://shelvia-w.github.io/blog/2024/new-challenge/" rel="alternate" type="text/html" title="New Challenge"/><published>2024-04-12T00:00:00+00:00</published><updated>2024-04-12T00:00:00+00:00</updated><id>https://shelvia-w.github.io/blog/2024/new-challenge</id><content type="html" xml:base="https://shelvia-w.github.io/blog/2024/new-challenge/"><![CDATA[<p>This is rather random, but as my PhD journey is coming close to an end, I would like to challenge myself with reading and coding. I am partially unsatisfied with where I am right now, but I think it‚Äôs not necessarily a bad thing, as it motivates me to improve myself. So, I will take a little bit of time every day to read and/or code things that I am interested in. In the field of AI, I am most interested in how to build trustworthy AI, so this includes topics like fairness, explainability, privacy, etc. But I‚Äôm also interested in other fields, such as particle physics and neuroscience. So I should challenge myself to improve my knowledge in those fields as well. I will try my best to log my journey daily, but I don‚Äôt want to put too much pressure on myself. I‚Äôll keep reminding myself that something is better than nothing, and at least I‚Äôm slowly leveling up.</p>]]></content><author><name></name></author><category term="challenge"/><category term="challenge,"/><category term="reading,"/><category term="coding"/><summary type="html"><![CDATA[üéØ A new challenge I set up for myself.]]></summary></entry></feed>