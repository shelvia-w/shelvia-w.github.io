<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://shelvia-w.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shelvia-w.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-04-13T19:17:17+00:00</updated><id>https://shelvia-w.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Day 1 of Learning</title><link href="https://shelvia-w.github.io/blog/2024/day1/" rel="alternate" type="text/html" title="Day 1 of Learning"/><published>2024-04-13T00:00:00+00:00</published><updated>2024-04-13T00:00:00+00:00</updated><id>https://shelvia-w.github.io/blog/2024/day1</id><content type="html" xml:base="https://shelvia-w.github.io/blog/2024/day1/"><![CDATA[<p>This is day 1 of the challenge to read/code more. I realized that this challenge is harder than I thought. But it’s kinda fun as I get to learn about so many interesting topics. Here’s a summary of what I learnt today:</p> <h3 id="brain-teaser-have-we-met-before">Brain teaser: Have we met before?</h3> <p>I published an article on <a href="https://medium.com/@shelvia1039/brain-teaser-26-have-we-met-before-cafd776b81d4">Medium</a> today about the theorem of friends and strangers. There’s actually a <a href="https://en.wikipedia.org/wiki/Theorem_on_friends_and_strangers#:~:text=The%20theorem%20on%20friends%20and,blue%20colours%20of%20each%20graph.">Wikipedia</a> page about it. The solution to this problem is related to <a href="https://en.wikipedia.org/wiki/Ramsey%27s_theorem">Ramsey’s theorem</a>.</p> <h3 id="leetcode-longest-substring-without-repeating-characters">LeetCode: Longest substring without repeating characters</h3> <p>I did a <a href="https://leetcode.com/problems/longest-substring-without-repeating-characters/description/">LeetCode</a> problem about finding the longest substring without repeating characters. I managed to solve it but was not quite satisfied with my solution. The given solution uses sliding window, which is an important concept in algorithms (I think). I am not too familiar with data structures and algorithms, so doing LeetCode gives me the opportunity to learn about them. Solving LeetCode is kinda like solving math puzzles as well, but currently I’m just doing easy problems to understand the common algorithms.</p> <h3 id="openai-improving-their-fine-tuning-api-and-custom-models-program">OpenAI: Improving their fine-tuning API and custom models program</h3> <p>OpenAI published in their <a href="https://openai.com/blog/introducing-improvements-to-the-fine-tuning-api-and-expanding-our-custom-models-program">blog</a> that they have new features for their fine-tuning API, including epoch-based checkpoint creation, comparative playground, third-party integration, comprehensive validation metrics, hyperparameter configuration and fine-tuning dashboard improvements. They are also expanding their custom models program to include assisted fine-tuning where you can work with their technical teams to fine-tuning the models more deeply to suit your interests. If you have large proprietary data, you may also work with them to integrate the domain knowledge better into the model for very specific use cases. I would love to read more about their retrieval-augmented generation (RAG), fine-tuning API and custom models program in the future.</p> <h3 id="generalization-in-deep-learning">Generalization in deep learning</h3> <p>I’ve been working with the topic of generalization in deep learning for a while now. But I’ve been struggling to consolidate all of them, and I haven’t been keeping up with the latest development. So I thought I should start somewhere and decided to start with a paper which I’ve been wanting to read: <a href="https://arxiv.org/pdf/1710.05468.pdf">Generalization in Deep Learning</a>. I’ve only read the Introduction so far, and here’s what I gather:</p> <ul> <li>Theoretical properties of neural networks include: expressivity and non-convex optimization (how trainable the deep hypothesis spaces are)</li> <li>Expressive and trainable hypothesis space does not guarantee generalization (due to possibility of overfitting)</li> <li>Classical theory suggests that: (a) generalization has someething to do with low-capacity hypothesis class (compact representation), and (b) deep hypothesis spaces have exponential advantage over shallow ones (need to read and understand this further)</li> <li>However, it has been shown that successful deep hypothesis spaces have sufficient capacity to memorize random labels (apparent paradox)</li> <li>Question: why do deep learning models generalize well despite their overwhelming capacity?</li> </ul> <h3 id="tmlr-nov-2023---mar-2024-and-jmlr-vol-25">TMLR (Nov 2023 - Mar 2024) and JMLR (Vol 25)</h3> <p>I picked some papers from <a href="https://jmlr.org/tmlr/papers/">TMLR</a> (Nov 2023 - Mar 2024) and <a href="https://jmlr.org/papers/v25/">JMLR</a> (Vol 25) which I find interesting. I found that there are quite a few papers analyzing the theoretical properties of gradient descent-based methods. Here are the top 3 papers which caught my attention:</p> <ul> <li><a href="https://jmlr.org/papers/volume25/19-301/19-301.pdf">On Truthing Issues in Supervised Classification</a></li> <li><a href="https://openreview.net/pdf?id=EWv9XGOpB3">Variational Classification: A Probabilistic Generalization of the Softmax Classifier</a></li> <li><a href="https://openreview.net/pdf?id=TQfQUksaC8">Pathologies of Predictive Diversity in Deep Ensembles</a> I’ll find some time to read about them…</li> </ul>]]></content><author><name></name></author><category term="challenge"/><category term="leetcode,"/><category term="openai,"/><category term="generalization,"/><category term="jmlr,"/><category term="tmlr"/><summary type="html"><![CDATA[LeetCode, OpenAI, Generalization, TMLR/JMLR]]></summary></entry><entry><title type="html">New Challenge</title><link href="https://shelvia-w.github.io/blog/2024/new-challenge/" rel="alternate" type="text/html" title="New Challenge"/><published>2024-04-12T00:00:00+00:00</published><updated>2024-04-12T00:00:00+00:00</updated><id>https://shelvia-w.github.io/blog/2024/new-challenge</id><content type="html" xml:base="https://shelvia-w.github.io/blog/2024/new-challenge/"><![CDATA[<p>This is rather random, but as my PhD journey is coming close to an end, I would like to challenge myself with reading and coding. I am partially unsatisfied with where I am right now, but I think it’s not necessarily a bad thing, as it motivates me to improve myself. So, I will take a little bit of time every day to read and/or code things that I am interested in. In the field of AI, I am most interested in how to build trustworthy AI, so this includes topics like fairness, explainability, privacy, etc. But I’m also interested in other fields, such as particle physics and neuroscience. So I should challenge myself to improve my knowledge in those fields as well. I will try my best to log my journey daily, but I don’t want to put too much pressure on myself. I’ll keep reminding myself that something is better than nothing, and at least I’m slowly leveling up.</p>]]></content><author><name></name></author><category term="challenge"/><category term="challenge,"/><category term="reading,"/><category term="coding"/><summary type="html"><![CDATA[A new challenge I set up for myself.]]></summary></entry></feed>