<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://shelvia-w.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shelvia-w.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-28T09:52:21+00:00</updated><id>https://shelvia-w.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">GPT-4o Just Leveled Up Its Image Generation!</title><link href="https://shelvia-w.github.io/blog/2025/4o_image/" rel="alternate" type="text/html" title="GPT-4o Just Leveled Up Its Image Generation!"/><published>2025-03-27T00:00:00+00:00</published><updated>2025-03-27T00:00:00+00:00</updated><id>https://shelvia-w.github.io/blog/2025/4o_image</id><content type="html" xml:base="https://shelvia-w.github.io/blog/2025/4o_image/"><![CDATA[<p>I‚Äôm so excited to try out <a href="https://openai.com/index/introducing-4o-image-generation/">4o‚Äôs new image generation</a>, I saw some cool ones from X, like <a href="https://x.com/WeAreNearYou/status/1905049046558920748">memes in Ghibli style</a>.</p> <h3 id="text-rendering">Text Rendering</h3> <p>Let‚Äôs first explore the ‚ÄúText Rendering‚Äù feature. Here‚Äôs my prompt:</p> <blockquote> <p>I‚Äôm starting a thriller/mystery book club called ‚ÄúBooked for Murder‚Äù. I want you to design an image - a poster incorporating an illustration of the attached book item (The Family Experiment) in cartoon style. Lean into the harry potter style and font while keeping it feeling upscale and sleek. Create a nice background that suits the theme. Make sure all the text is rendered correctly. Include a tag ‚Äúfree entry‚Äù.</p> <p>Join Our First <br/> Booked for Murder <br/> Book Club</p> <p>Book Selection <br/> (Cartoon Illustration of the Book)</p> <p>Sunday 30th March 2025 <br/> 15:00 - 16:00 <br/> The Book Cafe</p> </blockquote> <p>The result:</p> <p><img src="https://raw.githubusercontent.com/shelvia-w/shelvia-w.github.io/refs/heads/master/assets/img/book_club_poster_1.png" alt="Book Club Poster" width="420px"/></p> <p>Then I asked to change into gothic style and make the overall font smaller:</p> <p><img src="https://raw.githubusercontent.com/shelvia-w/shelvia-w.github.io/refs/heads/master/assets/img/book_club_poster_2.png" alt="Book Club Poster" width="420px"/></p> <p>I am actually impressed that it could render the text in the book so clearly as well.</p> <h3 id="character-consistency">Character Consistency</h3> <p>To test the ‚Äúconsistency‚Äù of the multi-turn generation, I first provided the following prompt:</p> <blockquote> <p>Create a concept art for a game character. The character is a girl with a white persian cat. The girl has a cute cat paw gloves and cat ears. The girl has a bubbly personality and is agile.</p> </blockquote> <p>The result:</p> <p><img src="https://raw.githubusercontent.com/shelvia-w/shelvia-w.github.io/refs/heads/master/assets/img/game_character_1.png" alt="Game Character" width="420px"/></p> <p>I don‚Äôt quite like the art style so I asked to make her cuter and use pixar-like art style:</p> <p><img src="https://raw.githubusercontent.com/shelvia-w/shelvia-w.github.io/refs/heads/master/assets/img/game_character_2.png" alt="Game Character" width="420px"/></p> <p>Then, I wanted to try to see how she will look like as a character in one of my favourite games (Overwatch):</p> <p><img src="https://raw.githubusercontent.com/shelvia-w/shelvia-w.github.io/refs/heads/master/assets/img/game_character_3.png" alt="Game Character" width="420px"/></p> <p>After that, I asked to turn it into a landscape image 16:9 ratio, and showed me a nice visual of a game where this character is strolling on a Japanese street:</p> <p><img src="https://raw.githubusercontent.com/shelvia-w/shelvia-w.github.io/refs/heads/master/assets/img/game_character_4.png" alt="Game Character" width="620px"/></p> <p>Lastly, I asked to create a profile interface with active quests:</p> <p><img src="https://raw.githubusercontent.com/shelvia-w/shelvia-w.github.io/refs/heads/master/assets/img/game_character_5.png" alt="Game Character" width="620px"/></p> <p>Overall, I think it‚Äôs quite consistent! That said, I‚Äôve noticed it occasionally alters the art style or facial features slightly.</p> <h3 id="upload-and-restyle">Upload and Restyle</h3> <p>I tried to restyle these photos of Valorant teams:</p> <p><img src="https://raw.githubusercontent.com/shelvia-w/shelvia-w.github.io/refs/heads/master/assets/img/vct_manga_1.png" alt="VCT Manga" width="420px"/></p> <p>with the following prompt:</p> <blockquote> <p>Create a black and white manga style of the photo. Keep the 16:9 aspect ratio.</p> </blockquote> <p>The result:</p> <p><img src="https://raw.githubusercontent.com/shelvia-w/shelvia-w.github.io/refs/heads/master/assets/img/vct_manga_2.png" alt="VCT Manga" width="420px"/></p> <p>While the overall image is accurate, certain details - such as glasses, height, or facial expressions - can sometimes be lost.</p> <h3 id="detailed-directions">Detailed Directions</h3> <p>I generated 9 items which I love using the following prompt:</p> <blockquote> <p>A square image containing a 3 row by 3 column grid containing 16 objects on a white background. Go from left to right, top to bottom. The objects should be in illustration style. Here‚Äôs the list:</p> <ol> <li>white dog</li> <li>pink heart</li> <li>cute gaming mouse and keyboard</li> <li>football</li> <li>cinnamoroll</li> <li>cherry blossom</li> <li>one piece anime</li> <li>hot matcha</li> <li>tempura</li> </ol> </blockquote> <p>The result:</p> <p><img src="https://raw.githubusercontent.com/shelvia-w/shelvia-w.github.io/refs/heads/master/assets/img/9_things_i_love.png" alt="9 Things I Love" width="420px"/></p> <p>What can I say, they are all spot on!</p> <h3 id="transparent-layers">Transparent Layers</h3> <p>I tried to generate Valorant stickers by providing these images:</p> <p><img src="https://raw.githubusercontent.com/shelvia-w/shelvia-w.github.io/refs/heads/master/assets/img/vct_stickers_1.png" alt="VCT Stickers" width="420px"/></p> <p>and the following prompt:</p> <blockquote> <p>Turn this into cute sticker, transparent background.</p> </blockquote> <p>The result:</p> <p><img src="https://raw.githubusercontent.com/shelvia-w/shelvia-w.github.io/refs/heads/master/assets/img/vct_stickers_2.png" alt="VCT Stickers" width="420px"/></p> <p>They‚Äôre so cute, omg! Most of them came out great on the first try, but a few needed a couple of runs to match my preferences.</p> <h3 id="infographics">Infographics</h3> <p>I tried to generate human anatomy infographic with the following prompt:</p> <blockquote> <p>Make a detailed visual infographic of human anatomy.</p> </blockquote> <p>The result:</p> <p><img src="https://raw.githubusercontent.com/shelvia-w/shelvia-w.github.io/refs/heads/master/assets/img/infographics_1.png" alt="Infographic" width="420px"/></p> <p>Honestly, I don‚Äôt think it‚Äôs 100% accurate, but the art looks nice.</p> <p>Then I continued to generate PC parts infographic with the following prompt:</p> <blockquote> <p>Make a detailed visual infographic of PC parts.</p> </blockquote> <p>The result:</p> <p><img src="https://raw.githubusercontent.com/shelvia-w/shelvia-w.github.io/refs/heads/master/assets/img/infographics_2.png" alt="Infographic" width="420px"/></p> <p>I feel like this one is quite detailed and accurate.</p> <p>I wanted more descriptions, so I asked to generate infographic to explain why sleep is important.</p> <blockquote> <p>Create an infographic explaining why sleep is important. Use cute art style.</p> </blockquote> <blockquote> <p><img src="https://raw.githubusercontent.com/shelvia-w/shelvia-w.github.io/refs/heads/master/assets/img/infographics_3.png" alt="Infographic" width="420px"/></p> </blockquote> <p>Lastly, I wanted to create a poster showing different types of flowers.</p> <blockquote> <p>Create an educational poster of different types of flowers in a vibrant watercolor style.</p> </blockquote> <p>The result:</p> <blockquote> <p><img src="https://raw.githubusercontent.com/shelvia-w/shelvia-w.github.io/refs/heads/master/assets/img/infographics_4.png" alt="Infographic" width="420px"/></p> </blockquote> <p>Overall, the infographics look great and definitely have the potential to be used in schools.</p> <h3 id="limitations">Limitations</h3> <p>Here are some limitations that they listed:</p> <ul> <li><strong>Cropping</strong>: occasionally crop longer images, like posters, too tightly, especially near the bottom (something that I encountered as well)</li> <li><strong>Hallucinations</strong>: make up information, especially in low-context prompts (definitely still a problem!)</li> <li><strong>High binding problems</strong>: may struggle to accurately render more than 10-20 distinct concepts at once, such as a full periodic table (tried to generate Standard Model elementary particles and it was struggling as well)</li> <li><strong>Precise graphing</strong>: (no description given, I didn‚Äôt try to generate data plots but I guess it can struggle to generate the correct, consistent tick labels)</li> <li><strong>Multilingual text rendering</strong>: struggles with rendering non-Latin languages, and the characters can be inaccurate or hallucinated, especially with more complexity (I‚Äôve only tried to generate English texts, but I can see the difficulties)</li> <li><strong>Editing precision</strong>: requests to edit specific portions of an image generation, such as typos are not always effective and may also alter other parts of the image in a way that was not requested or introduce more errors (YES, I felt the same - it‚Äôs so hard to edit the generated picture sometimes that I‚Äôd rather just regenerate it instead!)</li> <li><strong>Dense information with small text</strong>: struggle when asked to render detail information at a very small size (I did encounter this, but it‚Äôs understandable why the model will struggle with more texts to render)</li> </ul> <p>Overall, most of my prompts are rather short and simple. I guess if you‚Äôre really particular about the details, you could test how well it follows prompts by giving it longer, more specific ones.</p> <h3 id="references">References</h3> <ul> <li><a href="https://openai.com/index/introducing-4o-image-generation/">Introducing 4o Image Generation</a></li> </ul>]]></content><author><name></name></author><category term="news"/><category term="openai"/><summary type="html"><![CDATA[ü§î Text rendering? Nailed it. Prompt-following? Spot on. It's a huge upgrade!]]></summary></entry><entry><title type="html">Top AI Companies - What‚Äôs Buzzing? üêù</title><link href="https://shelvia-w.github.io/blog/2025/news/" rel="alternate" type="text/html" title="Top AI Companies - What‚Äôs Buzzing? üêù"/><published>2025-03-24T00:00:00+00:00</published><updated>2025-03-24T00:00:00+00:00</updated><id>https://shelvia-w.github.io/blog/2025/news</id><content type="html" xml:base="https://shelvia-w.github.io/blog/2025/news/"><![CDATA[<h3 id="meta">Meta</h3> <p>Llama models are being used to:</p> <ul> <li><a href="https://jobsearchgenius.ai/">build an AI career coach to improve job search outcomes</a></li> <li>build a tourism app to learn about local history and culture</li> <li><a href="https://fynopsis.ai/">facilitate more efficient and accurate deals in the mergers and acquisitions (M&amp;A) space</a> From what I gathered, some prefer open-source models due to cost efficiency (fixed cost instead of paying per API call), data security and privacy (as the model can be fine-tuned locally), and a large developer community (making it easier to find solutions to problems). I am honestly very keen on trying them out as well.</li> </ul> <h3 id="google-deepmind">Google DeepMind</h3> <p>Google DeepMind introduced <a href="https://deepmind.google/technologies/gemini-robotics/">two new robotics AI models</a>, based on Gemini 2.0: Gemini Robotics (advanced vision-language-action (VLA) model) and Gemini Robotics-ER (advanced spatial understanding). They have released a <a href="https://storage.googleapis.com/deepmind-media/gemini-robotics/gemini_robotics_report.pdf">Tech Report</a> for Gemini Robotics as well as a <a href="https://asimov-benchmark.github.io/">new ASIMOV dataset</a> for evaluating and improving semantic safety in these robotic models. It‚Äôs a fun read if you‚Äôre into robotics!</p> <h3 id="openai">OpenAI</h3> <p>Researchers at the MIT Media Lab and OpenAI conducted a series of studies to understand how AI use that involves emotional engagement (they called it ‚Äúaffective use‚Äù) can impact users‚Äô well-being. They carried out two studies: <a href="https://www.media.mit.edu/publications/investigating-affective-use-and-emotional-well-being-on-chatgpt/">an observational study</a> to analyze real-world on-platform usage patterns, and <a href="https://www.media.mit.edu/publications/how-ai-and-human-behaviors-shape-psychosocial-effects-of-chatbot-use-a-longitudinal-controlled-study/">a controlled interventional study</a> to understand the impacts on users. I‚Äôve always been fascinated by psychology and behavioral sciences, so this study truly captivates me!</p> <h3 id="microsoft">Microsoft</h3> <p>Microsoft released a playbook, <a href="https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/presentations/CSR/Accelerating-Sustainability-with-AI-2025.pdf">Accelerating sustainability with AI: Innovations for a better future</a>, outlining 5 ways to advance sustainability. I think it‚Äôs an interesting read for anyone interested in integrating AI into their business.</p> <h3 id="nvidia">NVIDIA</h3> <p>NVIDIA released <a href="https://huggingface.co/collections/nvidia/physical-ai-67c643edbb024053dcbcd6d8">Open Physical AI Dataset</a> (available on Hugging Face) for robotics and autonomous vehicle development. This dataset will continue to grow over time and will include both real-world and synthetic data. NVIDIA also will be using this dataset to train, test and validate physical AI for the <a href="https://nvidianews.nvidia.com/news/nvidia-announces-major-release-of-cosmos-world-foundation-models-and-physical-ai-data-tools">NVIDIA Cosmos</a> world model development platform, the <a href="https://www.nvidia.com/en-us/self-driving-cars/in-vehicle-computing/">NVIDIA DRIVE AV</a> software stack, the <a href="https://www.nvidia.com/en-us/industries/robotics/">NVIDIA Isaac</a> AI robot development platform and the <a href="https://www.nvidia.com/en-us/autonomous-machines/intelligent-video-analytics-platform/">NVIDIA Metropolis</a> application framework for smart cities. I‚Äôd love to dive deeper into each one of them, they sound extremely fascinating!</p> <h3 id="ibm">IBM</h3> <p>IBM released <a href="https://beeai.dev/">Bee AI</a> which is an open platform to run popular open-source AI agents from different frameworks. It can also be used to build specialized agents and be configured to work alone or with AI teammates. They also introduced <a href="https://docs.beeai.dev/acp/alpha/introduction">agent communication protocol</a> (ACP) to standardize how agents talk to each other. This is a step ahead of Anthropic‚Äôs <a href="https://modelcontextprotocol.io/introduction">model context protocol</a> (MCP) that standardizes how agents connect to tools and data to interact with and accomplish tasks in the real world. I think such protocol is important when developing multiagent systems, which are likely the next big advancement.</p> <h3 id="anthropic">Anthropic</h3> <p>Anthropic released a paper: <a href="https://assets.anthropic.com/m/317564659027fb33/original/Auditing-Language-Models-for-Hidden-Objectives.pdf">Auditing Language Models for Hidden Objectives</a>. In the paper, they studied the feasibilit of conducting alignment audits which are systematic investigations into whether models are pursuing hidden objectives. The objective is to uncover whether some AI systems that appear well-behaved actually harbor secret motives that are potentially misaligned with our intent. This is definitely something that I‚Äôll read up more in detail as I‚Äôm very much into AI safety!</p> <h3 id="references">References</h3> <ul> <li><a href="https://ai.meta.com/blog/built-with-llama-writesea-fynopsis-srimoyee-mukhopadhyay-united-states-economy/">Our open source Llama models are helping to spur economic growth in the US</a></li> <li><a href="https://deepmind.google/discover/blog/gemini-robotics-brings-ai-into-the-physical-world/">Gemini Robotics brings AI into the physical world</a></li> <li><a href="https://openai.com/index/affective-use-study/">Early methods for studying affective use and emotional well-being on ChatGPT</a></li> <li><a href="https://www.microsoft.com/en-us/microsoft-cloud/blog/2025/03/18/harnessing-ai-for-resilience-efficiency-and-sustainability/">Harnessing AI for resilience, efficiency, and sustainability</a></li> <li><a href="https://blogs.nvidia.com/blog/open-physical-ai-dataset/">NVIDIA Unveils Open Physical AI Dataset to Advance Robotics and Autonomous Vehicle Development</a></li> <li><a href="https://research.ibm.com/blog/multiagent-bee-ai">BeeAI now has multiple agents, and a standardized way for them to talk</a></li> <li><a href="https://www.anthropic.com/research/auditing-hidden-objectives">Auditing language models for hidden objectives</a></li> </ul>]]></content><author><name></name></author><category term="news"/><category term="meta,"/><category term="deepmind,"/><category term="openai,"/><category term="microsoft,"/><category term="nvidia,"/><category term="ibm,"/><category term="anthropic"/><summary type="html"><![CDATA[ü§î Meta's Llama Uses, DeepMind's Gemini Robotics, OpenAI's Emotional Engagement Study, NVIDIA's Physical AI Dataset, IBM's Bee AI, Anthropic's Alignment Audits]]></summary></entry><entry><title type="html">Transformers Can Work Without Normalization Layer?</title><link href="https://shelvia-w.github.io/blog/2025/dynamic_tanh/" rel="alternate" type="text/html" title="Transformers Can Work Without Normalization Layer?"/><published>2025-03-19T00:00:00+00:00</published><updated>2025-03-19T00:00:00+00:00</updated><id>https://shelvia-w.github.io/blog/2025/dynamic_tanh</id><content type="html" xml:base="https://shelvia-w.github.io/blog/2025/dynamic_tanh/"><![CDATA[<h3 id="dynamic-tanh">Dynamic Tanh</h3> <p>Last week, a friend shared an interesting paper with me from LeCun‚Äôs group, ‚Äú<a href="https://arxiv.org/pdf/2503.10622">Transformers without Normalization</a>,‚Äù where they introduced Dynamic Tanh (DyT) as a replacement for layer normalization in Transformers. Instead of using a normalization layer, they propose a simple element-wise operation:</p> \[DyT(x)=\tanh(\alpha x)\] <p>Their motivation? While researchers have been busy tweaking various parts of the Transformer architecture, the normalization layer has remained largely untouched. This raises an intriguing question: Is layer normalization truly essential for Transformers to perform well?</p> <h3 id="layer-normalization">Layer Normalization</h3> <p>Transformers typically rely on layer normalization (LN), which normalizes each token individually across its features, without considering other tokens or batch samples. LN is usually applied before self-attention and feed-forward layers. In the paper, they plotted the input-output curve of the LN layer (before the affine transformation) and noticed an interesting trend:</p> <ul> <li>In earlier layers, the relationship is mostly linear.</li> <li>In deeper layers, the curve starts resembling a tanh function.</li> </ul> <p>This observation aligns with insights from <a href="https://arxiv.org/pdf/2406.01255">several papers</a> exploring why normalization techniques work so well - definitely something I should dive deeper into!</p> <h3 id="results">Results</h3> <p>They found that swapping LN with DyT in Transformers resulted in a similar loss curve and comparable performance (slightly better in some cases). However, this doesn‚Äôt hold when replacing batch normalization with DyT in traditional architectures like ResNets, suggesting that the role of normalization might differ across model types.</p> <h3 id="questions-to-ponder">Questions to Ponder</h3> <ul> <li>Why is the suppression of extreme values or the non-linearity component crucial?</li> <li>Can we find an alternative function for batch normalization in classic networks that achieves similar results?</li> </ul> <h3 id="references">References</h3> <ul> <li><a href="https://arxiv.org/pdf/2503.10622">Transformers without Normalization</a></li> <li><a href="https://arxiv.org/pdf/2406.01255">On the Nonlinearity of Layer Normalization</a></li> </ul>]]></content><author><name></name></author><category term="paper"/><category term="regularization"/><summary type="html"><![CDATA[ü§î Yann LeCun's group introduced an alternative called Dynamic Tanh (DyT).]]></summary></entry><entry><title type="html">Sakana‚Äôs AI Scientist-Generated Research Papers Reviewed at ICLR 2025 Workshop</title><link href="https://shelvia-w.github.io/blog/2025/sakana_ai/" rel="alternate" type="text/html" title="Sakana‚Äôs AI Scientist-Generated Research Papers Reviewed at ICLR 2025 Workshop"/><published>2025-03-14T00:00:00+00:00</published><updated>2025-03-14T00:00:00+00:00</updated><id>https://shelvia-w.github.io/blog/2025/sakana_ai</id><content type="html" xml:base="https://shelvia-w.github.io/blog/2025/sakana_ai/"><![CDATA[<h3 id="sakana-ai">Sakana AI</h3> <p>I first heard about Sakana AI from a friend a week or two ago, and their <a href="https://sakana.ai/ai-scientist/">AI Scientist</a> immediately caught my attention. This AI agent isn‚Äôt just another research assistant‚Äîit generates novel ideas, writes code, runs experiments, visualizes results, and even composes full scientific papers, complete with a simulated review process for evaluation. After briefly reading the AI Scientist paper, I was intrigued by its potential and, since it‚Äôs open-source, I‚Äôm already thinking about how to implement it in my own field. The cost of generating these papers is surprisingly low‚Äîaround $15 per paper. Using just a single 8x NVIDIA H100 machine, they generated hundreds of papers in a week ‚Äî something that would have definitely made my supervisor very happy if I could do the same!</p> <h3 id="ai-generated-paper">AI-Generated Paper</h3> <p><a href="https://sakana.ai/ai-scientist-first-publication/">The 3 papers submitted to the ICLR 2025 workshop were generated by The AI Scientist-v2</a>, which was an improved version of the original AI Scientist, although the full details on the new model have yet to be released. The ICLR workshop name is ‚ÄúI Can‚Äôt Believe It‚Äôs Not Better: Challenges in Applied Deep Learning‚Äù. I think this is an interesting workshop which focuses on the challenges and failure modes of deep learning models. Of course, the organizers are fully aware that the paper was AI generated as Sakana AI has previously seeked permission to ‚Äútest‚Äù this experiment. The reviewers were only told that they might be reviewing AI generated papers (3 out of 43 papers) but were not told which ones. One of the papers, titled ‚ÄúCompositional Regularization: Unexpected Obstacles in Enhancing Neural Network Generalization‚Äù, received a score of 6.33 which is above the acceptance threshold. Nevertheless, the paper was eventually withdrawn after the reviewing process as it‚Äôs still unclear if AI-generated papers should be accepted at these venues. They also noted that none of the three papers met the threshold for acceptance in the ICLR main conference track. An interesting idea being brought up is using the AI Scientist to automate the reproducibility of existing papers instead of just generating new ones. Reproducibility is super important, but still pretty lacking in the research community, so this could actually be a game-changer. Also,looks like there‚Äôs another AI-generated paper accepted at the Tiny Papers workshop track at ICLR, this time from an AI agent called <a href="https://www.autoscience.ai/blog/meet-carl-the-first-ai-system-to-produce-academically-peer-reviewed-research">Carl</a>. Unlike Sakana‚Äôs AI Scientist, though, this one still had some human intervention.</p> <h3 id="some-issues-with-ai-scientist-generated-paper">Some Issues with AI Scientist-Generated Paper</h3> <ul> <li>Experiment details can sometimes be incorrect.</li> <li>Related work is incomplete and overly general.</li> <li>Cites incorrect references.</li> <li>Lacks precision in technical mathematical details.</li> <li>Figure captions can be inaccurate.</li> <li>Claims are not always clearly supported by the presented evidence and often lack further explanation.</li> <li>Has a tendency to overclaim.</li> </ul> <h3 id="questions-to-ponder">Questions to Ponder</h3> <ol> <li>Should AI-generated papers be submitted to the same venues as human-written ones, or do they need a separate category?</li> <li>If reviewers can‚Äôt distinguish between AI-generated and human-written papers, does it really matter how they were created?</li> <li>Are AI-generated papers just combining existing ideas, or can they truly create something novel?</li> </ol> <h3 id="references">References</h3> <ul> <li><a href="https://sakana.ai/ai-scientist/">The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</a></li> <li><a href="https://sakana.ai/ai-scientist-first-publication/">The AI Scientist Generates its First Peer-Reviewed Scientific Publication</a></li> <li><a href="https://www.autoscience.ai/blog/meet-carl-the-first-ai-system-to-produce-academically-peer-reviewed-research">Meet Carl: The First AI System To Produce Academically Peer-Reviewed Research</a></li> <li><a href="https://github.com/SakanaAI/AI-Scientist-ICLR2025-Workshop-Experiment/blob/master/compositional-regularization/annotated_paper.pdf">Compositional Regularization: Unexpected Obstacles in Enhancing Neural Network Generalization</a></li> </ul>]]></content><author><name></name></author><category term="news,"/><category term="paper"/><category term="sakana_ai"/><summary type="html"><![CDATA[ü§î Can AI truly conduct its own research and write a full paper?]]></summary></entry></feed>