<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>This is day 1 of the challenge to read/code more. I realized that this challenge is harder than I thought. But it’s kinda fun as I get to learn about so many interesting topics. Here’s a summary of what I learnt today:</p> <p><strong>Brain teaser: Have we met before?</strong></p> <p>I published an article on <a href="https://medium.com/@shelvia1039/brain-teaser-26-have-we-met-before-cafd776b81d4" rel="external nofollow noopener" target="_blank">Medium</a> today about the theorem of friends and strangers. There’s actually a <a href="https://en.wikipedia.org/wiki/Theorem_on_friends_and_strangers#:~:text=The%20theorem%20on%20friends%20and,blue%20colours%20of%20each%20graph." rel="external nofollow noopener" target="_blank">Wikipedia</a> page about it. The solution to this problem is related to <a href="https://en.wikipedia.org/wiki/Ramsey%27s_theorem" rel="external nofollow noopener" target="_blank">Ramsey’s theorem</a>.</p> <p><strong>LeetCode: Longest substring without repeating characters</strong></p> <p>I did a <a href="https://leetcode.com/problems/longest-substring-without-repeating-characters/description/" rel="external nofollow noopener" target="_blank">LeetCode</a> problem about finding the longest substring without repeating characters. I managed to solve it but was not quite satisfied with my solution. The given solution uses sliding window, which is an important concept in algorithms (I think). I am not too familiar with data structures and algorithms, so doing LeetCode gives me the opportunity to learn about them. Solving LeetCode is kinda like solving math puzzles as well, but currently I’m just doing easy problems to understand the common algorithms.</p> <p><strong>OpenAI: Improving their fine-tuning API and custom models program</strong></p> <p>OpenAI published in their <a href="https://openai.com/blog/introducing-improvements-to-the-fine-tuning-api-and-expanding-our-custom-models-program" rel="external nofollow noopener" target="_blank">blog</a> that they have new features for their fine-tuning API, including epoch-based checkpoint creation, comparative playground, third-party integration, comprehensive validation metrics, hyperparameter configuration and fine-tuning dashboard improvements. They are also expanding their custom models program to include assisted fine-tuning where you can work with their technical teams to fine-tuning the models more deeply to suit your interests. If you have large proprietary data, you may also work with them to integrate the domain knowledge better into the model for very specific use cases. I would love to read more about their retrieval-augmented generation (RAG), fine-tuning API and custom models program in the future.</p> <p><strong>Generalization in deep learning</strong></p> <p>I’ve been working with the topic of generalization in deep learning for a while now. But I’ve been struggling to consolidate all of them, and I haven’t been keeping up with the latest development. So I thought I should start somewhere and decided to start with a paper which I’ve been wanting to read: <a href="https://arxiv.org/pdf/1710.05468.pdf" rel="external nofollow noopener" target="_blank">Generalization in Deep Learning</a>. I’ve only read the Introduction so far, and here’s what I gather:</p> <ul> <li>Theoretical properties of neural networks include: expressivity and non-convex optimization (how trainable the deep hypothesis spaces are)</li> <li>Expressive and trainable hypothesis space does not guarantee generalization (due to possibility of overfitting)</li> <li>Classical theory suggests that: (a) generalization has someething to do with low-capacity hypothesis class (compact representation), and (b) deep hypothesis spaces have exponential advantage over shallow ones (need to read and understand this further)</li> <li>However, it has been shown that successful deep hypothesis spaces have sufficient capacity to memorize random labels (apparent paradox)</li> <li>Question: why do deep learning models generalize well despite their overwhelming capacity?</li> </ul> <p><strong>TMLR (Nov 2023 - Mar 2024) and JMLR (Vol 25)</strong></p> <p>I picked some papers from <a href="https://jmlr.org/tmlr/papers/" rel="external nofollow noopener" target="_blank">TMLR</a> (Nov 2023 - Mar 2024) and <a href="https://jmlr.org/papers/v25/" rel="external nofollow noopener" target="_blank">JMLR</a> (Vol 25) which I find interesting. I found that there are quite a few papers analyzing the theoretical properties of gradient descent-based methods. Here are the top 3 papers which caught my attention:</p> <ul> <li><a href="https://jmlr.org/papers/volume25/19-301/19-301.pdf" rel="external nofollow noopener" target="_blank">On Truthing Issues in Supervised Classification</a></li> <li><a href="https://openreview.net/pdf?id=EWv9XGOpB3" rel="external nofollow noopener" target="_blank">Variational Classification: A Probabilistic Generalization of the Softmax Classifier</a></li> <li><a href="https://openreview.net/pdf?id=TQfQUksaC8" rel="external nofollow noopener" target="_blank">Pathologies of Predictive Diversity in Deep Ensembles</a></li> </ul> </body></html>